<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>RWKV-v4 Web Demo</title>
  <script src="github-pages-coop-coep-workaround.js"></script> <!-- This allows us to use wasm threads on Github Pages. -->
</head>
<body>
  <h1>RWKV-v4 Web Demo</h1>
  <p>See <a href="https://github.com/josephrocca/rwkv-v4-web" target="_blank">the Github repo</a> for more details about this demo. The inference speed numbers are just for my laptop - consider them as relative numbers at most. Obviously varies by device. Note that opening the browser console/DevTools currently slows down inference, even after you close it.</p>
  
  <hr>

  <div id="modelChoiceEl">
    <div style="max-width:750px;">Choose a model:</div>
    <table id="modelTableEl">
      <tr style="font-weight:bold;">
        <td>Name</td>
        <td>Params</td>
        <td>File size</td>
        <td>Inference speed</td>
        <td>Notes</td>
      </tr>
      <tr>
        <td>rwkv-4-pile-169m.onnx</td>
        <td>169m</td>
        <td>679 MB</td>
        <td>~32 tokens/sec</td>
        <td>-</td>
        <td> <button onclick="modelUrlInputEl.value='https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m.onnx'; modelNumLayersEl.value='12'; modelEmbedDimEl.value='768'; backendEl.value='wasm'; loadBtnEl.click();">load</button> </td>
      </tr>
      <tr>
        <td>rwkv-4-pile-169m-uint8.onnx</td>
        <td>169m</td>
        <td>171 MB</td>
        <td>~12 tokens/sec</td>
        <td>uint8 quantized - smaller but slower</td>
        <td> <button onclick="modelUrlInputEl.value='https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m-uint8.onnx'; modelNumLayersEl.value='12'; modelEmbedDimEl.value='768'; backendEl.value='wasm'; loadBtnEl.click();">load</button> </td>
      </tr>
      <tr>
        <td>rwkv-4-pile-169m-webgl.onnx</td>
        <td>169m</td>
        <td>680 MB</td>
        <td>~16 tokens/sec</td>
        <td><a href="https://github.com/BlinkDL/RWKV-LM/issues/7#issuecomment-1225906768" target="_blank">webgl-compatible</a></td>
        <td> <button onclick="modelUrlInputEl.value='https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m-webgl.onnx'; modelNumLayersEl.value='12'; modelEmbedDimEl.value='768'; backendEl.value='webgl'; loadBtnEl.click();">load</button> </td>
      </tr>
      <tr style="opacity:0.3;">
        <td>rwkv-4-pile-430m.onnx</td>
        <td>430m</td>
        <td>1.73 GB</td>
        <td>?</td>
        <td style="color:red;">"RuntimeError: Aborted()" - too big to init</td>
        <td> <button onclick="modelUrlInputEl.value='https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m.onnx'; modelNumLayersEl.value='24'; modelEmbedDimEl.value='1024'; backendEl.value='wasm'; loadBtnEl.click();">load</button> </td>
      </tr>
      <tr>
        <td>rwkv-4-pile-430m.with_runtime_opt.ort</td>
        <td>430m</td>
        <td>1.73 GB</td>
        <td>~12 tokens/sec</td>
        <td>ORT format</td>
        <td> <button onclick="modelUrlInputEl.value='https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m.with_runtime_opt.ort'; modelNumLayersEl.value='24'; modelEmbedDimEl.value='1024'; backendEl.value='wasm'; loadBtnEl.click();">load</button> </td>
      </tr>
      <tr>
        <td>rwkv-4-pile-430m-uint8.onnx</td>
        <td>430m</td>
        <td>434 MB</td>
        <td>~4 tokens/sec</td>
        <td>uint8 quantized - smaller but slower</td>
        <td> <button onclick="modelUrlInputEl.value='https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m-uint8.onnx'; modelNumLayersEl.value='24'; modelEmbedDimEl.value='1024'; backendEl.value='wasm'; loadBtnEl.click();">load</button> </td>
      </tr>
      <tr>
        <td>rwkv-4-pile-430m-webgl.onnx</td>
        <td>430m</td>
        <td>1.73 GB</td>
        <td>~10 tokens/sec</td>
        <td><a href="https://github.com/BlinkDL/RWKV-LM/issues/7#issuecomment-1225906768" target="_blank">webgl-compatible</a></td>
        <td> <button onclick="modelUrlInputEl.value='https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m-webgl.onnx'; modelNumLayersEl.value='24'; modelEmbedDimEl.value='1024'; backendEl.value='webgl'; loadBtnEl.click();">load</button> </td>
      </tr>
    </table>
    <style>
      #modelTableEl {
        border-collapse: collapse;
        margin-top: 1rem;
      }
      #modelTableEl td, #modelTableEl th {
        border: 1px solid grey;
      }
    </style>
    <div style="max-width:750px; margin-top:1rem;">You can also load a custom model using a Hugging Face model URL. The URL must be a <u>direct</u> link to the model file. I.e. it must contain <b>/resolve/</b>, not /blob/ and must end in <i>.onnx</i> or <i>.ort</i></div>
    <div style="background:lightgrey; padding:0.5rem; width:fit-content;">
      <div><input id="modelUrlInputEl" style="width:700px" value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m-uint8.onnx"></div>
      <div>Num Layers: <input id="modelNumLayersEl" style="width:30px" value="12"></div>
      <div>Embed Dim: <input id="modelEmbedDimEl" style="width:30px" value="768"></div>
			<div>Backend: <select id="backendEl"> <option value="wasm">wasm</option> <option value="webgl">webgl (buggy)</option> </select></div>
      <button id="loadBtnEl" onclick="resolveLoadClick()">load custom model</button>
    </div>
  </div>

  <div id="loadingMessageEl" style="display:none;">
    <div id="loadingStatusEl">Loading...</div>
    <progress id="downloadProgressEl"></progress>
  </div>
   
  <div id="inputAreaEl" style="display:none;">
    <div id="modelInfoDisplayEl" style="opacity:0.5"></div>
    <select id="exampleSelectorEl" onchange="promptInputEl.value=this.value;" style="display:block;">
      <option value="The capital of Australia is Canberra.
The capital of France is Paris.
The capital of Peru is">country capitals</option>
      <option value="7*3=21
21+2=23
1+2*4=9
7*2+31=">basic math</option>
      <option value="QUESTION: The house is empty. Two people talk into the house. One person walks out, but then walks back in. A dog walks into the house. A person walks out of the house. How many people are in the house?
ANSWER: Let's think step by step. First,">word problem</option>
      <option value="Jamie: Hey, did you hear the news?
Sam: No, what happened?
Jamie: They're finally trialling gene drives to combat malaria in Nigeria!
Sam:">chat</option>
      <option value="dog --> fur --> soft --> teddy --> bear --> mammal --> taxonomy --> hierarchy --> king --> medieval -->">relationship chain</option>
    </select>
    <textarea id="promptInputEl" style="width:550px; height:250px;"></textarea>
    <script>
      promptInputEl.value = exampleSelectorEl.querySelectorAll("option")[0].value;
    </script>
    <div>Num output tokens (including prompt): <input id="numTokensToGenInputEl" type="number" value="128" style="width:60px;"> <span id="progressDisplayEl"></span> <span id="tokensPerSecDisplayEl" style="opacity:0.6"></span></div>
    <button id="generateButtonEl">generate text</button>
  </div>
  
  <script src="https://cdn.jsdelivr.net/pyodide/v0.21.0a2/full/pyodide.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js"></script>
  
  <script type="module">
    
    await new Promise(r => window.resolveLoadClick=r); // wait for load button click
    
    let onnxModelUrl = modelUrlInputEl.value;
    let n_layer = Number(modelNumLayersEl.value);
    let n_embd = Number(modelEmbedDimEl.value);
		let backend = backendEl.value;
    
    modelInfoDisplayEl.innerHTML = `<b>url:</b>${onnxModelUrl} <b>n_layer:</b>${n_layer} <b>n_embd:</b>${n_embd} <b>backend:</b>${backend}`;
    
    modelChoiceEl.style.display = "none";
    loadingMessageEl.style.display = "";

    window.pyodide = await loadPyodide();
    await pyodide.loadPackage("micropip");

    // Add tokenizer.json file to Pyodide filesystem:
    let tokenizerJsonText = await fetch("./tokenizer.json").then(r => r.text())
    pyodide.FS.writeFile("/tokenizer.json", tokenizerJsonText, { encoding: "utf8" });

    // Install tokenizer:
    console.log(await pyodide.runPythonAsync(`
      import sys
      print(sys.version)
      import os
      os.environ["TOKENIZERS_PARALLELISM"] = "0" # This is needed because threading doesn't work in Pyodide yet: https://github.com/pyodide/pyodide/issues/2816#issue-1290719241
      import micropip
      await micropip.install('./tokenizers_python-0.11.0-cp310-cp310-emscripten_3_1_14_wasm32.whl')
      from tokenizers import Tokenizer
      tokenizer = Tokenizer.from_file("/tokenizer.json")
    `));
    
    function textToTokens(text) {
      pyodide.globals.set("input_text", text);
      pyodide.runPython(`
        encoded = tokenizer.encode(input_text)
        ids = encoded.ids
        tokens = encoded.tokens
      `);
      // pyodide.globals.get('tokens').toJs()
      return pyodide.globals.get('ids').toJs();
    }
    
    function tokensToText(tokens) {
      pyodide.globals.set("input_tokens", tokens.join(","));
      pyodide.runPython(`
        input_tokens = [int(x) for x in input_tokens.split(',')]
        decoded = tokenizer.decode(input_tokens)
      `);
      return pyodide.globals.get('decoded');
    }
    
    function padLeftWithZeros(arr, size) {
      arr = arr.slice(0);

      while (arr.length < size) {
        arr.unshift(0);
      }

      return arr;
    }

    function greedySampling(x) {
      let max_k = 0;
      let max_v = x[0];

      for (let i = 1; i < 50277; i++) {
        if (x[i] > max_v) {
          max_v = x[i];
          max_k = i;
        }
      }

      return max_k;
    }


    function downloadBlobWithProgress(url, onProgress) {
      return new Promise((res, rej) => {
        var blob;
        var xhr = new XMLHttpRequest();
        xhr.open('GET', url, true);
        xhr.responseType = 'arraybuffer';
        xhr.onload = function(e) {
          blob = new Blob([this.response]);   
        };
        xhr.onprogress = onProgress;
        xhr.onloadend = function(e){
          res(blob);
        }
        xhr.send();
      });
    }


    let session;
    try {
      ort.env.wasm.proxy = true; // <-- When using wasm, proxy inference via a web worker so it doesn't freeze the main/rendering thread.
      
      if(self.crossOriginIsolated) { // needs to be cross-origin-isolated to use wasm threads. you need to serve this html file with these two headers: https://web.dev/coop-coep/
        ort.env.wasm.numThreads = navigator.hardwareConcurrency / 2;
      }
      
      ort.logLevel = "verbose";
      ort.logLevelInternal = "verbose";

      let sessionOptions = {
        executionProviders: [ backend ],
        graphOptimizationLevel: 'all',
      };
	    
      if(new URL(onnxModelUrl).pathname.endsWith(".ort")) {
        // See here for details: github.com/microsoft/onnxruntime/issues/13445#issuecomment-1430153341
        sessionOptions = {
          executionProviders: [ backend ],
          enableMemPattern: false,
          enableCpuMemArena: false,
          extra: {
            session: {
              disable_prepacking: "1",
              use_device_allocator_for_initializers: "0",
              use_ort_model_bytes_directly: "1",
              use_ort_model_bytes_for_initializers: "1",
            },
          },
        };
      }

      let onnxModelBlob = await downloadBlobWithProgress(onnxModelUrl, function(e) {
        let ratio = e.loaded / e.total;
        downloadProgressEl.value = ratio;
        loadingStatusEl.innerHTML = "Downloading..." + Math.round(ratio*e.total/1e6)+" MB";
      });

      loadingStatusEl.innerHTML = "Initializing...";
      
      let onnxModelBlobUrl = URL.createObjectURL(onnxModelBlob);

      session = await ort.InferenceSession.create(onnxModelBlobUrl, sessionOptions);
      
      URL.revokeObjectURL(onnxModelBlobUrl);
      
    } catch (e) {
      console.log(`Failed to load ONNX model: ${e}.`);
      console.error(e);
    }
    
    async function predictText(promptText, numTokensToGenerate=32, streamingCallback=null) {

      let startTime = Date.now();

      const xx_att_d = new Float32Array(n_layer*n_embd);
      const aa_att_d = new Float32Array(n_layer*n_embd);
      const bb_att_d = new Float32Array(n_layer*n_embd);
      const pp_att_d = new Float32Array(n_layer*n_embd);
      const xx_ffn_d = new Float32Array(n_layer*n_embd);

      pp_att_d.fill(-1e30);

      const xx_att = new ort.Tensor('float32', xx_att_d, [n_layer, n_embd]);
      const aa_att = new ort.Tensor('float32', aa_att_d, [n_layer, n_embd]);
      const bb_att = new ort.Tensor('float32', bb_att_d, [n_layer, n_embd]);
      const pp_att = new ort.Tensor('float32', pp_att_d, [n_layer, n_embd]);
      const xx_ffn = new ort.Tensor('float32', xx_ffn_d, [n_layer, n_embd]);

      // prepare feeds. use model input names as keys.
      let feeds = { idx: null, xx_att: xx_att, aa_att: aa_att, bb_att: bb_att, pp_att: pp_att, xx_ffn: xx_ffn };

      let promptTokens = textToTokens(promptText);
      let ctx = [ promptTokens.shift() ];

      // feed inputs and run
      for (let i = 0; i < numTokensToGenerate; i++) {
        let idx_d = Int32Array.from( padLeftWithZeros(ctx, 1024) );
        let idx = new ort.Tensor('int32', idx_d, [1024]);

        feeds.idx = idx;

        let results = await session.run(feeds);
        let token = greedySampling(results.x.data);

        if (promptTokens.length == 0) {
          if(streamingCallback) streamingCallback(token);
          ctx.push( token );
        } else {
          ctx.push( promptTokens.shift() );
        }

        feeds.xx_att = results.xx_att_r;
        feeds.aa_att = results.aa_att_r;
        feeds.bb_att = results.bb_att_r;
        feeds.pp_att = results.pp_att_r;
        feeds.xx_ffn = results.xx_ffn_r;

        progressDisplayEl.innerHTML = `Progress: ${i+1}/${numTokensToGenerate}`;
      }
      
      let timeTaken = Date.now() - startTime;
      console.log(`Finished. Took ${timeTaken}ms`);
      tokensPerSecDisplayEl.innerHTML = `(${(numTokensToGenerate/(timeTaken/1000)).toFixed(2)} tokens/sec)`;

      return tokensToText(ctx);
    }
    
    inputAreaEl.style.display = "";
    loadingMessageEl.style.display = "none";
    
    generateButtonEl.onclick = async function() {
      generateButtonEl.disabled = true;
      generateButtonEl.innerHTML = "Loading...";
      
      tokensPerSecDisplayEl.innerHTMl = "";
	    
      let promptText = promptInputEl.value;
      let numTokensToGenerate = Number(numTokensToGenInputEl.value);
      
      async function streamingCallback(token) {
        let text = tokensToText([token]);
        promptInputEl.value += text;
      }
      
      let result = await predictText(promptText, numTokensToGenerate, streamingCallback);
      
      console.log(result);
			
      generateButtonEl.disabled = false;
      generateButtonEl.innerHTML = "generate";
    };
    
  </script>
</body>
</html>
